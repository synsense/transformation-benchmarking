{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training spiking neural networks, *fast*\n",
    "\n",
    "Spiking neural networks (SNN) can be notoriously slow to train. A special case of recurrent neural network, they work with sequential inputs and rely on a form of gradient computation through time, which in the most common scenario is backpropagation through time. Given that events from event cameras or silicon cochlears have a temporal resolution of down to microseconds, the amount of time steps in data to train SNNs can easily go in the hundreds for a single sample. \n",
    "\n",
    "This would not be a problem if we trained on the extremely sparse data in continuous time directly, but the legacy of ANN machine learning frameworks has it that we have to work with dense tensors to train our SNN. That means that for a visual event stream input (think video) of spatial size (2, 128, 128) for channels, y and x we not only deal with some 10 frames per second but potentially hundreds per second, which increases input dimensions by a lot. \n",
    "\n",
    "When training a neural network of any kind, one might think about how the learning rate or model size affect training time. But when it comes to training *faster*, optimizing data movement is crucial. 3 out of the first 4 points in [this list](https://www.reddit.com/r/MachineLearning/comments/kvs1ex/d_here_are_17_ways_of_making_pytorch_training/) weighted after potential speed-up have to do with how data is shaped and moved around between actual computations. It makes a huge difference, because training faster with the same hardware means getting results faster, and being able to iterate quicker.\n",
    "\n",
    "For this post we train an SNN using [Sinabs](https://github.com/synsense/sinabs) based on PyTorch and surrogate gradients, which means that in the forward pass we use the heavily quantized output of spiking layers but in the backward pass we use a smoother surrogate function based on the internal state of the neurons. We'll use the [Heidelberg Spiking Speech Commands](https://zenkelab.org/resources/spiking-heidelberg-datasets-shd/) dataset to train our network to do audio stream classification. We'll benchmark different data loading strategies using [Tonic](https://github.com/neuromorphs/tonic) and show that with the right strategy, we can achieve a speed-up of up to XX times compared to a naïve strategy.\n",
    "\n",
    "For all our benchmarks, we already assume multiple worker loading threads and pinning the GPU memory. We'll increase throughput by using different forms of caching to disk or GPU. By applying deterministic transformations upfront and saving the new tensor, we can save a lot of time during training. \n",
    "\n",
    "All data from neuromorphic datasets in Tonic is provided as NxD numpy arrays. We'll need to transform this into a tensor to bring it to the GPU, and we'll also do some downsampling of time steps. Let's define the transforms for dense and sparse tensors. We know that the input data has 700 channels and about 0.8-1.2s samples with microsecond resolution. We'll downsample each sample to 100 channels, bin every 4 ms to one frame and cut samples that are longer than 1s. That leaves us with a maximum of 250 time steps per sample.\n",
    "\n",
    "<!-- Accelerators such as TPUs, Cerebra and the like heavily -->\n",
    "\n",
    "<!-- The reason why EXODUS is efficient is because it vectorizes samples in time. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tonic import transforms\n",
    "\n",
    "dt = 4000  # all time units in Tonic in us\n",
    "encoding_dim = 100\n",
    "\n",
    "dense_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Downsample(spatial_factor=encoding_dim / 700),\n",
    "        transforms.CropTime(max=1e6),\n",
    "        transforms.ToFrame(\n",
    "            sensor_size=(encoding_dim, 1, 1), time_window=dt, include_incomplete=True\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll import the training dataset and assign the respective transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tonic\n",
    "\n",
    "dense_dataset = tonic.datasets.SSC(\"./data\", split=\"train\", transform=dense_transform)\n",
    "print(f\"This dataset has {len(dense_dataset)} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give an idea of how one sample now looks like, let's print one dense and one sparse tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_sample, dense_target = dense_dataset[0]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(dense_sample.squeeze().T)\n",
    "plt.xlabel(\"Time step\")\n",
    "plt.ylabel(\"Channel\")\n",
    "plt.title(dense_dataset.classes[dense_target]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naïve dataloading\n",
    "\n",
    "We start with the first benchmark, where we load every sample from an hdf5 file on disk which provides us with a numpy array in memory. For each sample, we apply the [ToFrame](https://tonic.readthedocs.io/en/main/reference/generated/tonic.transforms.ToFrame.html) transform (defined earlier) to create a dense array which we can then batch together with other samples and feed it to the network.\n",
    "\n",
    "![naive caching](images/caching1.png \"Naive caching\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tonic\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader_kwargs = dict(\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    "    collate_fn=tonic.collation.PadTensors(batch_first=True),\n",
    ")\n",
    "\n",
    "naive_dataloader = DataLoader(dense_dataset, **dataloader_kwargs, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data, target in tqdm(naive_dataloader):\n",
    "    data, target = data.squeeze().cuda(), target.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's about 6-7 iterations/s, which is not very exciting. We haven't even started training yet! \n",
    "\n",
    "## Disk caching\n",
    "Let's try to be a bit smarter now. ToFrame is a deterministic transform, so for the same sample we'll always receive the same transformed data. Given that we might train for 100 epochs, which looks at each sample 100 times, that's a lot of wasted compute! Now we're going to cache, which means save, those transformed sampled to disk during the first epoch, so that we don't need to recompute them!\n",
    "\n",
    "![Disk caching](images/caching2.png \"Disk caching\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disk_cached_dataset = tonic.DiskCachedDataset(\n",
    "    dataset=dense_dataset,\n",
    "    cache_path=f\"cache/{dense_dataset.__class__.__name__}/train/{encoding_dim}/{dt}\",\n",
    ")\n",
    "\n",
    "disk_cached_dataloader = DataLoader(disk_cached_dataset, **dataloader_kwargs, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data, target in tqdm(disk_cached_dataloader):\n",
    "    data, target = data.squeeze().cuda(), target.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8-9 iterations/s is slower than before, what happened? In the first epoch, the caching will likely slow down the training. But let's see what happens in the second epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data, target in tqdm(disk_cached_dataloader):\n",
    "    data, target = data.squeeze().cuda(), target.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "32 iterations/s? Now this is faster! Every epoch from now on will load data at this speed, at the expense of disk space. How much disk space does it cost you may ask? Let's compare the size of the original dataset and the cache folder..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "size_orig_dataset = (\n",
    "    sum(f.stat().st_size for f in Path(\"data\").glob(\"**/*.h5\") if f.is_file()) / 1e9\n",
    ")\n",
    "size_cache_folder = (\n",
    "    sum(f.stat().st_size for f in Path(\"cache\").glob(\"**/*\") if f.is_file()) / 1e9\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"The size of the original dataset file is {round(size_orig_dataset, 2)} GB compared to the generated cache folder with {round(size_cache_folder, 2)} GB.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is quite efficient. As a reminder, the original dataset in this case contained numpy events, whereas the cache folder contains dense tensors. We can compress the dense tensors that much because by default Tonic uses lightweight compression during caching. Disk caching is a generally applicable and will save you a lot of time in the long run.\n",
    "\n",
    "## GPU caching\n",
    "We can even go faster! Instead of loading dense tensors from disk, we can try to cram all our dataset onto the GPU! Now, the issue is that with dense tensors this wouldn't work as they would occupy too much memory. But events are already an efficient format right? So we'll store the events on the GPU as sparse tensors and then simply inflate them by calling to_dense() for each sample. This method is obviously bound by GPU memory so works with rather small datasets such as then one we're testing. However, once you're setup, you can train with _blazing_ speed. Let's have a look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Downsample(spatial_factor=encoding_dim / 700),\n",
    "        transforms.CropTime(max=1e6),\n",
    "        transforms.ToSparseTensor(\n",
    "            sensor_size=(encoding_dim, 1, 1), time_window=dt, include_incomplete=True\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "sparse_dataset = tonic.datasets.SSC(\n",
    "    \"./data\", split=\"train\", transform=sparse_transform, target_transform=torch.tensor\n",
    ")\n",
    "\n",
    "sparse_sample = sparse_dataset[0][0]\n",
    "\n",
    "print(sparse_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "target_list = []\n",
    "for data, targets in tqdm(disk_cached_dataloader):\n",
    "    data_list.append(data.squeeze().to_sparse().coalesce().cuda())\n",
    "    target_list.append(targets.byte().cuda())\n",
    "\n",
    "sparse_tensor_dataset = list(zip(data_list, target_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data, target in tqdm(sparse_tensor_dataset):\n",
    "    data.to_dense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last line returns instantly... 10k batches/s sound like something we'd like to work with. The dataset now occupies some 8-9GB of GPU memory, which is quite a lot for a dataset of this size. But the speed speaks for itself, so it might pay off to run your experiments on an earlier-generation GPU with more memory just to really crank up that utilisation percentage!\n",
    "\n",
    "\n",
    "## Fast training\n",
    "So far, all we've done is loading the data on the device. It would be good to see how fast we can really train a network on the task. We'll use again our three dataloaders (naïve version, disk-cached and GPU-cached) to look at actual training times. Let's start by defining a simple integrate-and-fire (IAF) feed-forward (Sequential) architecture using [Sinabs](https://sinabs.readthedocs.io):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import sinabs.layers as sl\n",
    "import sinabs.exodus.layers as el\n",
    "\n",
    "\n",
    "class SNN(nn.Sequential):\n",
    "    def __init__(self, backend, hidden_dim: int = 128):\n",
    "        assert backend == sl or backend == el\n",
    "        super().__init__(\n",
    "            nn.Linear(encoding_dim, hidden_dim),\n",
    "            backend.IAF(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            backend.IAF(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            backend.IAF(),\n",
    "            nn.Linear(hidden_dim, 35),\n",
    "        )\n",
    "\n",
    "\n",
    "sinabs_model = SNN(backend=el).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sinabs\n",
    "\n",
    "# sparse_tensor_dataloader = DataLoader(sparse_tensor_dataset, batch_size=None, num_workers=0)\n",
    "model = sinabs_model\n",
    "train_dataloader = disk_cached_dataloader # sparse_tensor_dataloader # sparse_tensor_dataset # \n",
    "# def training():\n",
    "optim = torch.optim.Adam(model.parameters())\n",
    "criterion = torch.nn.functional.cross_entropy\n",
    "\n",
    "train_loss = []\n",
    "for epoch in range(3):\n",
    "    for data, targets in tqdm(train_dataloader):\n",
    "        sinabs.reset_states(model)\n",
    "        optim.zero_grad()\n",
    "        data = data.cuda().squeeze()\n",
    "        targets = targets.cuda()\n",
    "        # data = data.to_dense()\n",
    "        # targets = targets.to_dense()\n",
    "        output = model(data)\n",
    "        loss = criterion(output.sum(1), targets)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        train_loss.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_loss = [loss.cpu().detach().numpy() for loss in train_loss]\n",
    "plt.plot(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('exodus-test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c021c926ddd302c6ed51a52d37f274a9ee4bffd9f811c11e9a0b313c48b15ad1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
